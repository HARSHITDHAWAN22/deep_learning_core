This folder holds hands-on, from-scratch implementations of essential neural network building blocks — all powered by NumPy.

The idea is simple: instead of using libraries like TensorFlow or PyTorch, you’ll actually *see* how neural networks think and learn by coding every step yourself — from forward passes to backpropagation and parameter tuning.

Inside, you'll find examples and code for:
- How information flows through a simple feedforward neural network (forward pass).
- How the model learns and adjusts itself using backpropagation.
- How weights and biases get updated through gradient descent.

It’s a peek under the hood — learning the *why* behind the *what* of modern deep learning.

