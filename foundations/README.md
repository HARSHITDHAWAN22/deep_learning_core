# Deep Learning Foundations

This folder contains **from-scratch implementations** of key concepts that form the core of deep learning.  
The goal is to develop a clear understanding of how optimization and learning mechanisms work internally by building everything manually using **NumPy**, without relying on machine learning frameworks.


## Topics Covered

- Gradient Descent built from scratch  
- Common loss functions (Mean Squared Error, Cross Entropy)  
- Activation functions (Sigmoid, ReLU, Tanh)  
- Linear Regression trained via Gradient Descent  
- Logistic Regression for binary classification (optional)  


## Purpose

These implementations aim to build a foundation in understanding:  
- How model parameters are optimized  
- How loss functions guide the learning process  
- How simple models learn patterns from data  

This knowledge provides the groundwork for exploring **neural networks** and deeper architectures with confidence.
