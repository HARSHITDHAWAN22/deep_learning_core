# Sequence Models Basics (RNN / LSTM)

This repository includes **from-scratch implementations** of fundamental sequence models using **NumPy**, focusing on **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** units.  

The aim is to build a clear understanding of how sequential data is processed over time and how hidden states enable models to retain information across time steps.


## Topics Covered

- Basic RNN cell implementation (forward pass)  
- LSTM cell forward pass with forget, input, and output gates  
- Simple sequence prediction on a toy dataset  


## Purpose

Sequence models are essential for handling data that comes in sequences â€” such as time-series signals, text, and speech.  
By implementing these models manually, this project focuses on:

- Understanding temporal dependencies in data  
- Tracking how information flows between time steps  
- Exploring how LSTMs manage long-term dependencies better than simple RNNs  

All implementations are built with **NumPy**, emphasizing **conceptual clarity** and step-by-step understanding rather than computational efficiency.
