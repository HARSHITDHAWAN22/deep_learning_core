# Neural Networks (From Scratch)


This folder includes **from-scratch implementations** of fundamental neural network components using **NumPy**.  
Rather than relying on high-level frameworks, every step of the learning process is built manually to provide a clear understanding of how neural networks function internally.


## Topics Covered

- Forward propagation in a basic feedforward network  
- Backpropagation for gradient computation  
- Weight and bias updates via gradient descent  
- Training a simple Multi-Layer Perceptron (MLP) on a toy dataset  
- Demonstrating overfitting on a small dataset  


## Purpose

Neural networks are often used as black boxes when working with modern frameworks.  
This module removes that abstraction and focuses on:

- How data moves through network layers  
- How errors propagate backward through the model  
- How parameters adjust to minimize loss  

By understanding these steps deeply, youâ€™ll gain stronger intuition for **designing, debugging, and improving** deep learning models.
